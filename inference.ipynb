{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2e8af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:27:59.808240Z",
     "iopub.status.busy": "2022-11-08T08:27:59.807355Z",
     "iopub.status.idle": "2022-11-08T08:28:41.435225Z",
     "shell.execute_reply": "2022-11-08T08:28:41.434106Z"
    },
    "papermill": {
     "duration": 41.636341,
     "end_time": "2022-11-08T08:28:41.437991",
     "exception": false,
     "start_time": "2022-11-08T08:27:59.801650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Downloading and Extracting Packages\r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "######################################################################## | 100% \r\n",
      "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n",
      "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\r\n",
      "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\r\n"
     ]
    }
   ],
   "source": [
    "!conda install --offline /kaggle/input/track4-train/*.tar.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7ef942d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:41.460762Z",
     "iopub.status.busy": "2022-11-08T08:28:41.459846Z",
     "iopub.status.idle": "2022-11-08T08:28:41.464680Z",
     "shell.execute_reply": "2022-11-08T08:28:41.463874Z"
    },
    "papermill": {
     "duration": 0.01807,
     "end_time": "2022-11-08T08:28:41.466592",
     "exception": false,
     "start_time": "2022-11-08T08:28:41.448522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !ls /kaggle/input/track4-train/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c540cfc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:41.487628Z",
     "iopub.status.busy": "2022-11-08T08:28:41.487361Z",
     "iopub.status.idle": "2022-11-08T08:28:41.492844Z",
     "shell.execute_reply": "2022-11-08T08:28:41.491910Z"
    },
    "papermill": {
     "duration": 0.018248,
     "end_time": "2022-11-08T08:28:41.494828",
     "exception": false,
     "start_time": "2022-11-08T08:28:41.476580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [0.6382310415028003, 0.6189348573669843, 0.594305478150372, 0.6456368418815037, 0.650946396469057, 0.6610530600887038]\n",
    "# 0.6348512792432368\n",
    "# Full validation metric: 0.6373838583629989\n",
    "\n",
    "# Counter({1: 47225, 0: 42655})\n",
    "# ROC AUC metric: 0.6831401875473259\n",
    "# Accuracy: 0.6177681352914998\n",
    "# Full target metric: 0.5901076765112288\n",
    "# Full target metric fixed: 0.5898723615115555\n",
    "# Target metric by center_id:\n",
    "# Center_id 1: 0.5837030889916592\n",
    "# Center_id 2: 0.5128716159021234\n",
    "# Center_id 3: 0.6234127532907505\n",
    "# Center_id 4: 0.5803120005321822\n",
    "# Center_id 5: 0.633916901390537\n",
    "# Center_id 6: 0.6509644227059665\n",
    "# Center_id 7: 0.5673793620392944\n",
    "# Center_id 8: 0.6490039016757818\n",
    "# Center_id 9: 0.529810187493138\n",
    "# Center_id 10: 0.5777616326658479\n",
    "# Center_id 11: 0.5891695748590473"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "099f8d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:41.517160Z",
     "iopub.status.busy": "2022-11-08T08:28:41.515587Z",
     "iopub.status.idle": "2022-11-08T08:28:41.520835Z",
     "shell.execute_reply": "2022-11-08T08:28:41.519942Z"
    },
    "papermill": {
     "duration": 0.017665,
     "end_time": "2022-11-08T08:28:41.522646",
     "exception": false,
     "start_time": "2022-11-08T08:28:41.504981",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# data = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/train.csv')\n",
    "# print(Counter(data['center_id'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6f497c2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:41.544892Z",
     "iopub.status.busy": "2022-11-08T08:28:41.544103Z",
     "iopub.status.idle": "2022-11-08T08:28:41.549892Z",
     "shell.execute_reply": "2022-11-08T08:28:41.549091Z"
    },
    "papermill": {
     "duration": 0.019313,
     "end_time": "2022-11-08T08:28:41.551978",
     "exception": false,
     "start_time": "2022-11-08T08:28:41.532665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BAD_IMAGE_IDS = ['5adc4c_0', '7b9aaa_0', 'bb06a5_0', 'e26a04_0', '280c26_0'] + \\\n",
    "                ['4ae44b_0', '53e66f_0', '7c2c2f_0', '74a450_1']\n",
    "\n",
    "BLOCK_SIZE = 28\n",
    "BLOCKS_PER_CROP = 8\n",
    "CROP_SIZE = BLOCK_SIZE * BLOCKS_PER_CROP\n",
    "BLOCK_THR = 90\n",
    "CROP_THR = 0.6\n",
    "MAX_CROPS_PER_IMAGE = 20\n",
    "IMAGES_PER_SAMPLE = 4\n",
    "EPOCHS_NUM = 10\n",
    "SCALE_FACTOR = 24\n",
    "TEST_SAMPLE_DUPL_RATE = 20\n",
    "TRAIN_SAMPLE_DUPL_RATE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f10734d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:41.573422Z",
     "iopub.status.busy": "2022-11-08T08:28:41.573170Z",
     "iopub.status.idle": "2022-11-08T08:28:43.344655Z",
     "shell.execute_reply": "2022-11-08T08:28:43.343679Z"
    },
    "papermill": {
     "duration": 1.785007,
     "end_time": "2022-11-08T08:28:43.347089",
     "exception": false,
     "start_time": "2022-11-08T08:28:41.562082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ClotImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_ids: List[str],\n",
    "            labels: List[str],\n",
    "            image_crops: List[List[np.ndarray]],\n",
    "            seed: int,\n",
    "            is_test: bool,\n",
    "            transformations,\n",
    "    ):\n",
    "        self.image_ids = image_ids\n",
    "        self.labels = [float(label == 'CE') for label in labels]\n",
    "        self.image_crops = image_crops\n",
    "        self.seed = seed\n",
    "        self.is_test = is_test\n",
    "        self.transformations = transformations\n",
    "\n",
    "        if not self.is_test:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "            label_to_indices = defaultdict(list)\n",
    "            for i, (label, crops) in enumerate(zip(self.labels, self.image_crops)):\n",
    "                if len(crops) > 0:\n",
    "                    label_to_indices[label].append(i)\n",
    "\n",
    "            max_size = TRAIN_SAMPLE_DUPL_RATE * max(len(indices) for indices in label_to_indices.values())\n",
    "\n",
    "            self.sample_ids = []\n",
    "            for i, indices in enumerate(label_to_indices.values()):\n",
    "                np.random.shuffle(indices)\n",
    "                while len(self.sample_ids) < max_size * (i + 1):\n",
    "                    req_size = min(len(indices), max_size * (i + 1) - len(self.sample_ids))\n",
    "                    self.sample_ids += indices[:req_size]\n",
    "        else:\n",
    "            self.sample_ids = []\n",
    "            for _ in range(TEST_SAMPLE_DUPL_RATE):\n",
    "                self.sample_ids.extend(list(range(len(self.image_ids))))\n",
    "\n",
    "        self.image_index_ids = []\n",
    "        sample_id_to_image_index = defaultdict(int)\n",
    "        for sample_id in self.sample_ids:\n",
    "            self.image_index_ids.append(sample_id_to_image_index[sample_id])\n",
    "            image_crops_cnt = len(self.image_crops[sample_id])\n",
    "            if image_crops_cnt:\n",
    "                sample_id_to_image_index[sample_id] = (sample_id_to_image_index[sample_id] + 1) % image_crops_cnt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_test:\n",
    "            np.random.seed(self.seed + idx)\n",
    "            random.seed(self.seed + idx)\n",
    "            torch.manual_seed(self.seed + idx)\n",
    "        idx, image_index = self.sample_ids[idx], self.image_index_ids[idx]\n",
    "        if len(self.image_crops[idx]) == 0:\n",
    "            return (\n",
    "                self.transformations(Image.fromarray(np.zeros((224, 224, 3)).astype(np.uint8))),\n",
    "                torch.tensor(self.labels[idx]),\n",
    "                self.image_ids[idx],\n",
    "            )\n",
    "        # image_index = np.random.randint(0, len(self.image_crops[idx]))\n",
    "        return (\n",
    "            self.transformations(self.image_crops[idx][image_index]),\n",
    "            torch.tensor(self.labels[idx]),\n",
    "            self.image_ids[idx],\n",
    "        )\n",
    "\n",
    "\n",
    "def get_loader(\n",
    "        image_ids: List[str],\n",
    "        labels: List[str],\n",
    "        image_crops: List[List[np.ndarray]],\n",
    "        seed: int,\n",
    "        is_test: bool,\n",
    "        transformations,\n",
    "        shuffle: bool,\n",
    "        batch_size: int,\n",
    "        num_workers: int\n",
    "):\n",
    "    dataset = ClotImageDataset(\n",
    "        image_ids, labels, image_crops, seed, is_test, transformations,\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(dataset, shuffle=shuffle, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19d00248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:43.369745Z",
     "iopub.status.busy": "2022-11-08T08:28:43.368729Z",
     "iopub.status.idle": "2022-11-08T08:28:43.706328Z",
     "shell.execute_reply": "2022-11-08T08:28:43.705417Z"
    },
    "papermill": {
     "duration": 0.35089,
     "end_time": "2022-11-08T08:28:43.708507",
     "exception": false,
     "start_time": "2022-11-08T08:28:43.357617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "from time import time\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyvips\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class DataPreparation:\n",
    "    def __init__(self, visualize: bool = False, seed: int = 42):\n",
    "        self.visualize = visualize\n",
    "        self.seed = seed\n",
    "\n",
    "        train_metadata = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/train.csv')\n",
    "        train_metadata = list(zip(\n",
    "            train_metadata['image_id'].tolist(),\n",
    "            train_metadata['label'].tolist(),\n",
    "            train_metadata['center_id'].tolist(),\n",
    "        ))\n",
    "        self.train = self._filter_bad_images(train_metadata)\n",
    "        self.all_center_ids = sorted(list({center_id for _, _, center_id in self.train}))\n",
    "\n",
    "        other_metadata = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/other.csv').query('label == \\'Other\\'')\n",
    "        other_metadata = list(zip(\n",
    "            other_metadata['image_id'].tolist(),\n",
    "            ['LAA' for _ in range(other_metadata.shape[0])],\n",
    "            [-1 for _ in range(other_metadata.shape[0])],\n",
    "        ))\n",
    "        self.other = self._filter_bad_images(other_metadata)\n",
    "        \n",
    "#         self.test = self.train\n",
    "        test_metadata = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/test.csv')\n",
    "        self.test = list(zip(\n",
    "            test_metadata['image_id'].tolist(),\n",
    "            ['Unknown' for _ in range(test_metadata.shape[0])],\n",
    "            test_metadata['center_id'].tolist(),\n",
    "        ))\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_bad_images(data: List[Tuple]) -> List[Tuple]:\n",
    "        return [\n",
    "            (image_id, label, center_id)\n",
    "            for image_id, label, center_id in data\n",
    "            if image_id not in BAD_IMAGE_IDS\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _add_rect_to_numpy(image: np.ndarray, x: int, y: int, size: int, thickness: int) -> None:\n",
    "        image[x:x + size, y:y + thickness] = (0, 0, 0)\n",
    "        image[x:x + thickness, y:y + size] = (0, 0, 0)\n",
    "        image[x:x + size, y + size:y + size + thickness] = (0, 0, 0)\n",
    "        image[x + size:x + size + thickness, y:y + size] = (0, 0, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_blocks_map(image: np.ndarray) -> np.ndarray:\n",
    "        pixels_diff = np.sum((image[:-1, :, :] - image[1:, :, :]) ** 2, axis=2)\n",
    "        pixels_diff = np.cumsum(np.cumsum(pixels_diff, axis=0), axis=1)\n",
    "        blocks_map = np.zeros((\n",
    "            (image.shape[0] + BLOCK_SIZE - 1) // BLOCK_SIZE,\n",
    "            (image.shape[1] + BLOCK_SIZE - 1) // BLOCK_SIZE,\n",
    "        ))\n",
    "        for x in range(0, pixels_diff.shape[0], BLOCK_SIZE):\n",
    "            for y in range(0, pixels_diff.shape[1], BLOCK_SIZE):\n",
    "                nx = min(x + BLOCK_SIZE, pixels_diff.shape[0])\n",
    "                ny = min(y + BLOCK_SIZE, pixels_diff.shape[1])\n",
    "                block_sum = int(pixels_diff[nx - 1, ny - 1])\n",
    "                if x:\n",
    "                    block_sum -= int(pixels_diff[x - 1, ny - 1])\n",
    "                if y:\n",
    "                    block_sum -= int(pixels_diff[nx - 1, y - 1])\n",
    "                if x and y:\n",
    "                    block_sum += int(pixels_diff[x - 1, y - 1])\n",
    "                blocks_map[x // BLOCK_SIZE][y // BLOCK_SIZE] = \\\n",
    "                    (block_sum / BLOCK_SIZE / BLOCK_SIZE) > BLOCK_THR\n",
    "        return blocks_map\n",
    "\n",
    "    def _generate_crops_positions(\n",
    "            self,\n",
    "            image: np.ndarray,\n",
    "            crop_thr: float,\n",
    "    ) -> Tuple[List[Tuple[int, int]], np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        blocks_map = self._get_blocks_map(image)\n",
    "\n",
    "        if self.visualize:\n",
    "            for i in range(blocks_map.shape[0]):\n",
    "                for j in range(blocks_map.shape[1]):\n",
    "                    if blocks_map[i][j]:\n",
    "                        self._add_rect_to_numpy(\n",
    "                            image,\n",
    "                            i * BLOCK_SIZE,\n",
    "                            j * BLOCK_SIZE,\n",
    "                            BLOCK_SIZE,\n",
    "                            1,\n",
    "                        )\n",
    "\n",
    "        good_crops_starts = []\n",
    "        for x in range(0, image.shape[0] - CROP_SIZE + 1, BLOCK_SIZE):\n",
    "            for y in range(0, image.shape[1] - CROP_SIZE + 1, BLOCK_SIZE):\n",
    "                _x, _y = x // BLOCK_SIZE, y // BLOCK_SIZE\n",
    "                crop_sum = blocks_map[_x:_x + BLOCKS_PER_CROP, _y:_y + BLOCKS_PER_CROP].sum()\n",
    "                if crop_sum > BLOCKS_PER_CROP * BLOCKS_PER_CROP * crop_thr:\n",
    "                    good_crops_starts.append((x, y))\n",
    "\n",
    "        if self.visualize:\n",
    "            for x, y in good_crops_starts:\n",
    "                self._add_rect_to_numpy(image, x, y, CROP_SIZE, 1)\n",
    "\n",
    "        return good_crops_starts\n",
    "\n",
    "    @staticmethod\n",
    "    def _process_crop(crop: np.ndarray) -> np.ndarray:\n",
    "        return crop\n",
    "\n",
    "    def _create_crops(\n",
    "        self,\n",
    "        image: np.ndarray,\n",
    "        crops_starts: List[Tuple[int]],\n",
    "    ) -> List[np.ndarray]:\n",
    "        return [\n",
    "            Image.fromarray(\n",
    "                self._process_crop(\n",
    "                    image[x:x + CROP_SIZE, y:y + CROP_SIZE],\n",
    "                )\n",
    "            )\n",
    "            for x, y in crops_starts\n",
    "        ]\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_unique_crops(crop_starts: List[Tuple[int, int]], order) -> List[Tuple[int, int]]:\n",
    "        def inter_size_1d(a: int, b: int, c: int, d: int) -> int:\n",
    "            return max(0, min(b, d) - max(a, c))\n",
    "\n",
    "        def inter_size_2d(crop_start_1: Tuple[int, int], crop_start_2: Tuple[int, int]) -> int:\n",
    "            return inter_size_1d(\n",
    "                crop_start_1[0], crop_start_1[0] + CROP_SIZE,\n",
    "                crop_start_2[0], crop_start_2[0] + CROP_SIZE,\n",
    "            ) * inter_size_1d(\n",
    "                crop_start_1[1], crop_start_1[1] + CROP_SIZE,\n",
    "                crop_start_2[1], crop_start_2[1] + CROP_SIZE,\n",
    "            )\n",
    "\n",
    "        crop_starts_sorted = sorted(crop_starts, key=order)\n",
    "        final_crop_starts = []\n",
    "        for crop_start in crop_starts_sorted:\n",
    "            if any(\n",
    "                    inter_size_2d(crop_start, crop_start_prev) > CROP_SIZE * CROP_SIZE // 2\n",
    "                    for crop_start_prev in final_crop_starts\n",
    "            ):\n",
    "                continue\n",
    "            final_crop_starts.append(crop_start)\n",
    "        return final_crop_starts\n",
    "    \n",
    "    @staticmethod\n",
    "    def _read_and_resize_image(image_id: str, base_image_path: str) -> np.ndarray:       \n",
    "        image_path = os.path.join(base_image_path, f'{image_id}.tif')\n",
    "        image = pyvips.Image.new_from_file(image_path, access='sequential')\n",
    "        return image.resize(1.0 / SCALE_FACTOR).numpy()\n",
    "    \n",
    "    def prepare_crops(\n",
    "            self,\n",
    "            image_ids: List[int],\n",
    "            base_image_path: str,\n",
    "    ) -> Tuple[List[List[np.ndarray]], List[List[Tuple[int]]], List[Tuple[np.ndarray, np.ndarray]]]:\n",
    "        np.random.seed(self.seed)\n",
    "        image_crops = []\n",
    "        image_crops_indices = []\n",
    "        for image_id in tqdm(image_ids):\n",
    "            start_time = time()\n",
    "            image = self._read_and_resize_image(image_id, base_image_path)\n",
    "            gc.collect()\n",
    "            print(f'Rescaling done in {time() - start_time} seconds. Image shape is {image.shape}')\n",
    "            found_flag = False\n",
    "            for crop_thr in np.arange(CROP_THR, -0.1, -0.1):\n",
    "                good_crops_starts = self._generate_crops_positions(image, crop_thr)\n",
    "                if len(good_crops_starts) < IMAGES_PER_SAMPLE:\n",
    "                    print('Bad image', image_id, 'crop_thr', crop_thr, 'only', len(good_crops_starts))\n",
    "                    continue\n",
    "\n",
    "                good_crops_starts_unique = []\n",
    "                for order in [\n",
    "                    lambda x: (x[0], x[1]),\n",
    "                    lambda x: (-x[0], -x[1]),\n",
    "                ]:\n",
    "                    good_crops_starts_unique.extend(self._get_unique_crops(good_crops_starts, order))\n",
    "                good_crops_starts_unique = list(set(good_crops_starts_unique))\n",
    "\n",
    "                if len(good_crops_starts_unique) < IMAGES_PER_SAMPLE:\n",
    "                    print('Bad image', image_id, 'crop_thr', crop_thr, 'only', len(good_crops_starts_unique))\n",
    "                    continue\n",
    "\n",
    "                good_crops_starts_sample_ids = np.random.choice(\n",
    "                    list(range(len(good_crops_starts_unique))),\n",
    "                    min(len(good_crops_starts_unique), MAX_CROPS_PER_IMAGE),\n",
    "                    replace=False,\n",
    "                )\n",
    "                good_crops_starts_sample = np.array(good_crops_starts_unique)[good_crops_starts_sample_ids]\n",
    "                image_crops_indices.append(good_crops_starts_sample)\n",
    "                image_crops.append(self._create_crops(image, good_crops_starts_sample))\n",
    "                found_flag = True\n",
    "                break\n",
    "            if not found_flag:\n",
    "                image_crops_indices.append([])\n",
    "                image_crops.append([])\n",
    "                print('No crops was found')\n",
    "            print(f'Done {image_id} in {time() - start_time} seconds')\n",
    "        gc.collect()\n",
    "        return image_crops, image_crops_indices\n",
    "\n",
    "    def process_train(\n",
    "            self\n",
    "    ) -> Tuple[List[List[np.ndarray]], List[List[Tuple[int]]]]:\n",
    "        return self.prepare_crops(\n",
    "            [image_id for image_id, _, _ in self.train],\n",
    "            '/kaggle/input/mayo-clinic-strip-ai/train/',\n",
    "        )\n",
    "\n",
    "    def process_other(\n",
    "            self\n",
    "    ) -> Tuple[List[List[np.ndarray]], List[List[Tuple[int]]]]:\n",
    "        return self.prepare_crops(\n",
    "            [image_id for image_id, _, _ in self.other],\n",
    "            '/kaggle/input/mayo-clinic-strip-ai/other/',\n",
    "        )\n",
    "\n",
    "    def process_test(\n",
    "        self\n",
    "    ) -> Tuple[List[List[np.ndarray]], List[List[Tuple[int]]]]:\n",
    "        return self.prepare_crops(\n",
    "            [image_id for image_id, _, _ in self.test],\n",
    "            '/kaggle/input/mayo-clinic-strip-ai/test',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c480439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:43.730375Z",
     "iopub.status.busy": "2022-11-08T08:28:43.729528Z",
     "iopub.status.idle": "2022-11-08T08:28:43.929310Z",
     "shell.execute_reply": "2022-11-08T08:28:43.928419Z"
    },
    "papermill": {
     "duration": 0.212769,
     "end_time": "2022-11-08T08:28:43.931592",
     "exception": false,
     "start_time": "2022-11-08T08:28:43.718823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    # transforms.RandomResizedCrop((224, 224), scale=(0.5, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=1.0),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    # transforms.RandomResizedCrop((224, 224), scale=(0.5, 1.0), ratio=(1.0, 1.0)),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=1.0),\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, saturation=0.5, hue=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "315c78b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:43.952784Z",
     "iopub.status.busy": "2022-11-08T08:28:43.952438Z",
     "iopub.status.idle": "2022-11-08T08:28:43.963772Z",
     "shell.execute_reply": "2022-11-08T08:28:43.962746Z"
    },
    "papermill": {
     "duration": 0.024391,
     "end_time": "2022-11-08T08:28:43.966105",
     "exception": false,
     "start_time": "2022-11-08T08:28:43.941714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def _group_by_patients(y_true, y_pred, image_ids):\n",
    "    patients = [image_id.split('_')[0] for image_id in image_ids]\n",
    "    patient_to_y_true, patient_to_y_pred = defaultdict(list), defaultdict(list)\n",
    "    for y, y_hat, patient in zip(y_true, y_pred, patients):\n",
    "        patient_to_y_true[patient].append(y)\n",
    "        patient_to_y_pred[patient].append(y_hat)\n",
    "    patient_to_y_true = {\n",
    "        patient: np.mean(y_true)\n",
    "        for patient, y_true in patient_to_y_true.items()\n",
    "    }\n",
    "    patient_to_y_pred = {\n",
    "        patient: np.mean(y_pred).tolist()\n",
    "        for patient, y_pred in patient_to_y_pred.items()\n",
    "    }\n",
    "    y_true, y_pred, patients = [], [], []\n",
    "    for patient, y in patient_to_y_true.items():\n",
    "        y_true.append(y)\n",
    "        y_pred.append(patient_to_y_pred[patient])\n",
    "        patients.append(patient)\n",
    "        \n",
    "    return y_true, np.array([[1 - p, p] for p in y_pred]), patients\n",
    "\n",
    "\n",
    "def get_target_metric(y_true, y_pred, image_ids):\n",
    "    return _weighted_mc_log_loss(*_group_by_patients(y_true, y_pred, image_ids)[:2])\n",
    "\n",
    "\n",
    "def _weighted_mc_log_loss(y_true, y_pred, epsilon=1e-15):\n",
    "    class_cnt = [sum(int(val == cl) for val in y_true) for cl in range(2)]\n",
    "    w = [0.5 for _ in range(2)]\n",
    "    return -sum(\n",
    "        w[cl] * sum(\n",
    "            (y == cl) / class_cnt[cl] * np.log(max(min(y_hat, 1 - epsilon), epsilon))\n",
    "            for y, y_hat in zip(y_true, y_pred[:, cl])\n",
    "        )\n",
    "        for cl in range(2)\n",
    "    ) / sum(w[cl] for cl in range(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eff5e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:43.987433Z",
     "iopub.status.busy": "2022-11-08T08:28:43.986575Z",
     "iopub.status.idle": "2022-11-08T08:28:43.996903Z",
     "shell.execute_reply": "2022-11-08T08:28:43.995966Z"
    },
    "papermill": {
     "duration": 0.023136,
     "end_time": "2022-11-08T08:28:43.998959",
     "exception": false,
     "start_time": "2022-11-08T08:28:43.975823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class ClotModelSingle(nn.Module):\n",
    "    def __init__(self, encoder_model):\n",
    "        super().__init__()\n",
    "\n",
    "        if encoder_model == 'effnet_b0':\n",
    "            base_model = models.efficientnet_b0(pretrained=True)\n",
    "            self.model = base_model.features\n",
    "            in_features_cnt = base_model.classifier[1].in_features\n",
    "        elif encoder_model == 'resnet18':\n",
    "            base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "            self.model = nn.Sequential(*list(base_model.children())[:-2])\n",
    "            in_features_cnt = list(base_model.children())[-1].in_features\n",
    "        elif encoder_model == 'regnet_x_1_6gf':\n",
    "            base_model = models.regnet_x_1_6gf(weights=models.RegNet_X_1_6GF_Weights.IMAGENET1K_V2)\n",
    "            self.model = nn.Sequential(base_model.stem, base_model.trunk_output)\n",
    "            in_features_cnt = base_model.fc.in_features\n",
    "        else:\n",
    "            raise Exception('Incorrect encoder name')\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features_cnt, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def freeze_encoder(self, flag):\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = not flag\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.model(x))\n",
    "\n",
    "    def save(self, model_path):\n",
    "        weights = self.state_dict()\n",
    "        torch.save(weights, model_path)\n",
    "\n",
    "    def load(self, model_path):\n",
    "        weights = torch.load(model_path, map_location='cpu')\n",
    "        self.load_state_dict(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "479bb38e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:44.020491Z",
     "iopub.status.busy": "2022-11-08T08:28:44.019661Z",
     "iopub.status.idle": "2022-11-08T08:28:44.026847Z",
     "shell.execute_reply": "2022-11-08T08:28:44.025987Z"
    },
    "papermill": {
     "duration": 0.020116,
     "end_time": "2022-11-08T08:28:44.028773",
     "exception": false,
     "start_time": "2022-11-08T08:28:44.008657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_final_prediction_with_train_data(submission: pd.DataFrame) -> pd.DataFrame:\n",
    "    train_data = pd.read_csv('/kaggle/input/mayo-clinic-strip-ai/train.csv')\n",
    "    patient_id_to_label = {\n",
    "        image_id.split('_')[0]: label\n",
    "        for image_id, label in zip(train_data['image_id'].tolist(), train_data['label'].tolist())\n",
    "    }\n",
    "    submission['CE'] = [\n",
    "        pred if patient_id not in patient_id_to_label else float(patient_id_to_label[patient_id] == 'CE')\n",
    "        for patient_id, pred in zip(submission['patient_id'].tolist(), submission['CE'].tolist())\n",
    "    ]\n",
    "    submission['LAA'] = [\n",
    "        pred if patient_id not in patient_id_to_label else float(patient_id_to_label[patient_id] == 'LAA')\n",
    "        for patient_id, pred in zip(submission['patient_id'].tolist(), submission['LAA'].tolist())\n",
    "    ]\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb3e09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-08T08:28:44.050196Z",
     "iopub.status.busy": "2022-11-08T08:28:44.049503Z",
     "iopub.status.idle": "2022-11-08T08:30:41.735904Z",
     "shell.execute_reply": "2022-11-08T08:30:41.734595Z"
    },
    "papermill": {
     "duration": 117.700549,
     "end_time": "2022-11-08T08:30:41.739202",
     "exception": false,
     "start_time": "2022-11-08T08:28:44.038653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling done in 44.03065800666809 seconds. Image shape is (2533, 1417, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:44<02:12, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 006388_0 in 44.31237602233887 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2/4 [00:47<00:39, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling done in 2.7464895248413086 seconds. Image shape is (1237, 248, 3)\n",
      "Done 008e5c_0 in 2.764355182647705 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:59<00:16, 16.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rescaling done in 12.525835037231445 seconds. Image shape is (2575, 636, 3)\n",
      "Done 00c058_0 in 12.608379125595093 seconds\n",
      "Rescaling done in 44.161428451538086 seconds. Image shape is (1106, 2326, 3)\n",
      "Bad image 01adc5_0 crop_thr 0.6 only 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:44<00:00, 26.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 01adc5_0 in 44.39000487327576 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 5/5 [00:06<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  patient_id        CE       LAA\n",
      "0     006388  0.469765  0.530235\n",
      "1     008e5c  0.519275  0.480725\n",
      "2     00c058  0.508980  0.491020\n",
      "3     01adc5  0.540291  0.459709\n",
      "  patient_id   CE  LAA\n",
      "0     006388  1.0  0.0\n",
      "1     008e5c  1.0  0.0\n",
      "2     00c058  0.0  1.0\n",
      "3     01adc5  0.0  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import ssl\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "cudnn.benchmark = True\n",
    "\n",
    "TEST_BATCH_SIZE = 16\n",
    "DUMPED_DATALOADER_PATH = '/kaggle/input/track-4-dataprep/data_loaders.pkl'\n",
    "\n",
    "\n",
    "def _get_model_path_by_center_id(folder_name: str, center_id: str) -> str:\n",
    "    for file_name in os.listdir(folder_name):\n",
    "        if not file_name.endswith('.h5'):\n",
    "            continue\n",
    "        if file_name.split('_')[2] == center_id:\n",
    "            return os.path.join(folder_name, file_name)\n",
    "    raise Exception(f'Model for center id {center_id} was not found in folder {folder_name}')\n",
    "\n",
    "\n",
    "def _explode_image_ids(image_ids: List[str], factor: int) -> List[str]:\n",
    "    exploded_image_ids = []\n",
    "    for start_id in range(0, len(image_ids), TEST_BATCH_SIZE):\n",
    "        end_id = min(start_id + TEST_BATCH_SIZE, len(image_ids))\n",
    "        for _ in range(factor):\n",
    "            exploded_image_ids.extend(image_ids[start_id:end_id])\n",
    "    return exploded_image_ids\n",
    "\n",
    "\n",
    "data_prep = DataPreparation()\n",
    "\n",
    "image_crops, _ = data_prep.process_test()\n",
    "# with open(DUMPED_DATALOADER_PATH, 'rb') as file:\n",
    "#     image_crops, _ = pickle.load(file)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataloader = get_loader(\n",
    "    [image_id for image_id, _, _ in data_prep.test],\n",
    "    [label for _, label, _ in data_prep.test],\n",
    "    image_crops,\n",
    "    seed=41,\n",
    "    is_test=True,\n",
    "    transformations=test_transforms,\n",
    "    shuffle=False,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "models = [\n",
    "    torch.load(_get_model_path_by_center_id(\n",
    "        '../input/track4-trains',\n",
    "        center_id,\n",
    "    ), map_location=torch.device('cpu')).to(device)\n",
    "    for center_id in ['1.5', '10.3', '11', '4', '6.2.8.9', '7']\n",
    "]\n",
    "for model in models:\n",
    "    model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat, y, image_ids = [], [], []\n",
    "    for image, label, image_id in tqdm(dataloader):\n",
    "        image = image.to(device)\n",
    "        label = label.cpu().detach().numpy().tolist()\n",
    "        for model in models:\n",
    "            y_hat.extend(model.forward(image).squeeze().cpu().detach().numpy().tolist())\n",
    "            y.extend(label)\n",
    "            image_ids.extend(image_id)\n",
    "            \n",
    "bad_image_ids = {\n",
    "    image_id\n",
    "    for image_id, crops in zip([image_id for image_id, _, _ in data_prep.test], image_crops)\n",
    "    if len(crops) == 0 \n",
    "}\n",
    "y_hat_fixed = [\n",
    "    0.5 if image_id in bad_image_ids else p\n",
    "    for p, image_id in zip(y_hat, image_ids)\n",
    "]\n",
    "\n",
    "labels, preds, patients = _group_by_patients(y, y_hat_fixed, image_ids)\n",
    "result = pd.DataFrame({\n",
    "    'patient_id': patients,\n",
    "    'CE': [pair[1].round(6) for pair in preds],\n",
    "    'LAA': [pair[0].round(6) for pair in preds],\n",
    "})\n",
    "print(result)\n",
    "result = update_final_prediction_with_train_data(result)\n",
    "print(result)\n",
    "result.to_csv('submission.csv', index=False)\n",
    "# print(Counter([int(p > 0.5) for p in y_hat_fixed]))\n",
    "# print('ROC AUC metric:', roc_auc_score(y, y_hat_fixed))\n",
    "# accuracy = sum([int(int(p > 0.5) == label) for p, label in zip(y_hat_fixed, y)]) / len(y)\n",
    "# print('Accuracy:', accuracy)\n",
    "# image_id_to_center_id = {image_id: center_id for image_id, _, center_id in data_prep.test}\n",
    "# target_metric = get_target_metric(\n",
    "#     y,\n",
    "#     y_hat,\n",
    "#     image_ids,\n",
    "# )\n",
    "# print('Full target metric:', target_metric)\n",
    "# target_metric = get_target_metric(\n",
    "#     y,\n",
    "#     y_hat_fixed,\n",
    "#     image_ids,\n",
    "# )\n",
    "# print('Full target metric fixed:', target_metric)\n",
    "# print('Target metric by center_id:')\n",
    "# for center_id in range(1, 12):\n",
    "#     sub_y = [label for label, image_id in zip(y, image_ids) if image_id_to_center_id[image_id] == center_id]\n",
    "#     sub_y_hat = [pred for pred, image_id in zip(y_hat_fixed, image_ids) if image_id_to_center_id[image_id] == center_id]\n",
    "#     sub_image_ids = [image_id for image_id in image_ids if image_id_to_center_id[image_id] == center_id]\n",
    "#     print(f'Center_id {center_id}: {get_target_metric(sub_y, sub_y_hat, sub_image_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d76962c",
   "metadata": {
    "papermill": {
     "duration": 0.010835,
     "end_time": "2022-11-08T08:30:41.762146",
     "exception": false,
     "start_time": "2022-11-08T08:30:41.751311",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 171.179908,
   "end_time": "2022-11-08T08:30:43.397394",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-11-08T08:27:52.217486",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
